{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar LangChain Integration\n",
    "\n",
    "Upstage Solar Chat API의 LangChain 통합 예시입니다.\n",
    "\n",
    "README.md의 예시와 동일한 코드입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SolarLLM (텍스트 완성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solar_langchain import SolarLLM\n",
    "\n",
    "# 인스턴스 생성\n",
    "llm = SolarLLM(reasoning_effort=\"medium\")\n",
    "\n",
    "# 단일 호출\n",
    "response = llm.invoke(\"파이썬의 장점을 설명해주세요\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍\n",
    "for chunk in llm.stream(\"한국의 사계절에 대해 알려주세요\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SolarChatModel (대화형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solar_langchain import SolarChatModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# 인스턴스 생성\n",
    "chat = SolarChatModel(reasoning_effort=\"high\")\n",
    "\n",
    "# 메시지 리스트로 대화\n",
    "messages = [\n",
    "    SystemMessage(content=\"당신은 친절한 AI 어시스턴트입니다.\"),\n",
    "    HumanMessage(content=\"안녕하세요!\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍\n",
    "for chunk in chat.stream([HumanMessage(content=\"짧은 이야기를 들려주세요\")]):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 체인과 함께 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solar_langchain import SolarChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = SolarChatModel()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 {role}입니다.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"role\": \"요리 전문가\",\n",
    "    \"question\": \"김치찌개 맛있게 끓이는 법을 알려주세요\"\n",
    "})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reasoning_effort 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solar_langchain import SolarLLM\n",
    "\n",
    "# 복잡한 문제에는 high 사용\n",
    "llm = SolarLLM(reasoning_effort=\"high\")\n",
    "response = llm.invoke(\"양자역학의 불확정성 원리를 설명해주세요\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 응답에는 low 사용\n",
    "llm = SolarLLM(reasoning_effort=\"low\")\n",
    "response = llm.invoke(\"안녕하세요!\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
